{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.estimation import estimate_text_distribution\n",
    "from src.MLE import MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each subject, estimate the distribution of human-written text and AI-generated text\n",
    "for name in [\"CS\",\"EESS\",\"Math\",\"Phys\",\"Stat\"]:\n",
    "    estimate_text_distribution(f\"data/training_data/{name}/human_data.parquet\",f\"data/training_data/{name}/ai_data.parquet\",f\"distribution/{name}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.000,     0.025,     0.002,     0.025\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.025,     0.054,     0.003,     0.029\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.050,     0.080,     0.004,     0.030\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.075,     0.101,     0.004,     0.026\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.100,     0.128,     0.004,     0.028\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.125,     0.151,     0.005,     0.026\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.150,     0.174,     0.005,     0.024\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.175,     0.194,     0.005,     0.019\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.200,     0.223,     0.005,     0.023\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.225,     0.244,     0.005,     0.019\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.250,     0.270,     0.006,     0.020\n",
      "=====================================\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.000,     0.026,     0.003,     0.026\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.025,     0.053,     0.003,     0.028\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.050,     0.077,     0.004,     0.027\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.075,     0.103,     0.004,     0.028\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.100,     0.130,     0.004,     0.030\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.125,     0.150,     0.005,     0.025\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.150,     0.172,     0.005,     0.022\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.175,     0.194,     0.005,     0.019\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.200,     0.220,     0.005,     0.020\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.225,     0.243,     0.005,     0.018\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m=\u001b[39mMLE(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistribution/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alpha \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0.025\u001b[39m,\u001b[38;5;241m0.05\u001b[39m,\u001b[38;5;241m0.075\u001b[39m,\u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;241m0.125\u001b[39m,\u001b[38;5;241m0.15\u001b[39m,\u001b[38;5;241m0.175\u001b[39m,\u001b[38;5;241m0.2\u001b[39m,\u001b[38;5;241m0.225\u001b[39m,\u001b[38;5;241m0.25\u001b[39m]:\n\u001b[0;32m----> 6\u001b[0m     estimated,ci\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/validation_data/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/ground_truth_alpha_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43malpha\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mabs\u001b[39m(estimated\u001b[38;5;241m-\u001b[39malpha)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGround Truth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/projects/retractions_nlp/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:163\u001b[0m, in \u001b[0;36mMLE.inference\u001b[0;34m(self, inference_file_path)\u001b[0m\n\u001b[1;32m    161\u001b[0m data \u001b[38;5;241m=\u001b[39m inference_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minference_sentence\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mset\u001b[39m(token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m x \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set))\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Infer the confidence interval for the mixing parameter alpha\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m confidence_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap_alpha_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# Calculate and round the mean of the confidence interval and its half-width\u001b[39;00m\n\u001b[1;32m    165\u001b[0m solution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mround\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(confidence_interval), \u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/projects/retractions_nlp/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:116\u001b[0m, in \u001b[0;36mMLE.bootstrap_alpha_inference\u001b[0;34m(self, data, n_bootstrap)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03mInfers the mixing parameter (alpha) between two distributions (P and Q) using bootstrap resampling.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    representing a 95% confidence interval for the mixing parameter alpha.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Precomputes log probabilities for each data point in the entire dataset\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m full_log_p_values, full_log_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecompute_log_probabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# To store estimated alpha values from each bootstrap sample\u001b[39;00m\n\u001b[1;32m    118\u001b[0m alpha_values_bootstrap \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/projects/retractions_nlp/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:87\u001b[0m, in \u001b[0;36mMLE.precompute_log_probabilities\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03mPrecomputes log probabilities for each data point in the dataset, according to both distributions P and Q.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03mwhen calculating the total log probability of sentences.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution P, i.e. generated by humans\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m log_p_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_p_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     88\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_p_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution Q, i.e. generated by AI\u001b[39;00m\n\u001b[1;32m     90\u001b[0m log_q_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_q_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     91\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_q_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[0;32m~/projects/retractions_nlp/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:88\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03mPrecomputes log probabilities for each data point in the dataset, according to both distributions P and Q.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03mwhen calculating the total log probability of sentences.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution P, i.e. generated by humans\u001b[39;00m\n\u001b[1;32m     87\u001b[0m log_p_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_p_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[0;32m---> 88\u001b[0m                 \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_one_minus_p_hat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_tokens_set\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution Q, i.e. generated by AI\u001b[39;00m\n\u001b[1;32m     90\u001b[0m log_q_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_q_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     91\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_q_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[0;32m~/projects/retractions_nlp/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:88\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03mPrecomputes log probabilities for each data point in the dataset, according to both distributions P and Q.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03mwhen calculating the total log probability of sentences.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution P, i.e. generated by humans\u001b[39;00m\n\u001b[1;32m     87\u001b[0m log_p_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_p_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[0;32m---> 88\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_p_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution Q, i.e. generated by AI\u001b[39;00m\n\u001b[1;32m     90\u001b[0m log_q_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_q_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     91\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_q_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for each subject, estimate the alpha value of mixed text and calculate the error\n",
    "for name in [\"CS\",\"EESS\",\"Math\",\"Phys\",\"Stat\"]:\n",
    "    # load the framework\n",
    "    model=MLE(f\"distribution/{name}.parquet\")\n",
    "    for alpha in [0,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.225,0.25]:\n",
    "        estimated,ci=model.inference(f\"data/validation_data/{name}/ground_truth_alpha_{alpha}.parquet\")\n",
    "        error=abs(estimated-alpha)\n",
    "        print(f\"{'Ground Truth':>10},{'Prediction':>10},{'CI':>10},{'Error':>10}\")\n",
    "        print(f\"{alpha:10.3f},{estimated:10.3f},{ci:10.3f},{error:10.3f}\")\n",
    "    print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_parquet(\"data/training_data/CS/human_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['human_sentence'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [particularly, the, proposed, spreading, curve...\n",
       "1        [this, mixed, approach, has, rarely, been, app...\n",
       "2        [firstly, observing, the, limited, rotation, i...\n",
       "3        [however, existing, methods, either, require, ...\n",
       "4        [in, this, task, a, fused, image, containing, ...\n",
       "                               ...                        \n",
       "37855    [active, learning, shows, promise, to, decreas...\n",
       "37856    [finally, simulations, using, flashflow, for, ...\n",
       "37857    [in, this, paper, we, model, this, intention, ...\n",
       "37858    [these, results, help, to, identify, the, most...\n",
       "37859    [intuitively, a, shorter, tree, with, pure, le...\n",
       "Name: human_sentence, Length: 37860, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"human_sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37860"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = pd.read_parquet(\"data/validation_data/CS/ground_truth_alpha_0.25.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "retraction_df = pd.read_parquet(\"../retraction_fulltext_dataset/24_08_22_retraction_with_text.gzip\")\n",
    "reference_df = pd.read_parquet(\"../retraction_fulltext_dataset/24_11_30_reference_articles.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Field\n",
       "Medicine                                        7658\n",
       "Biochemistry, Genetics and Molecular Biology    4914\n",
       "Computer Science                                3346\n",
       "Engineering                                     3337\n",
       "Environmental Science                           1564\n",
       "Social Sciences                                 1419\n",
       "Materials Science                                983\n",
       "Agricultural and Biological Sciences             968\n",
       "Business, Management and Accounting              822\n",
       "Neuroscience                                     723\n",
       "Chemistry                                        633\n",
       "Psychology                                       631\n",
       "Immunology and Microbiology                      554\n",
       "Earth and Planetary Sciences                     404\n",
       "Health Professions                               378\n",
       "Physics and Astronomy                            332\n",
       "Economics, Econometrics and Finance              326\n",
       "Decision Sciences                                278\n",
       "Mathematics                                      259\n",
       "Arts and Humanities                              221\n",
       "Energy                                           212\n",
       "Pharmacology, Toxicology and Pharmaceutics       175\n",
       "Nursing                                          173\n",
       "Dentistry                                        140\n",
       "Chemical Engineering                              73\n",
       "Veterinary                                        19\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retraction_df[\"Field\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Domain\n",
       "Physical Sciences    11143\n",
       "Health Sciences       8368\n",
       "Life Sciences         7334\n",
       "Social Sciences       3697\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retraction_df[\"Domain\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "retraction_df[\"OriginalPaperDate\"] = pd.to_datetime(retraction_df[\"OriginalPaperDate\"], format='%m/%d/%Y %H:%M')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = retraction_df[retraction_df[\"OriginalPaperDate\"]>pd.to_datetime('29.11.2022', format='%d.%m.%Y')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_loader import dataset_gpt_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "retraction_gpr_inferenece_dataset = dataset_gpt_inference(retraction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "retraction_gpt_inference_data = pd.DataFrame(pd.Series(retraction_gpr_inferenece_dataset), columns=[\"inference_sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [photodynamic, therapy, pdt, has, been, demons...\n",
       "1       [however, tumour, regrowth, may, occur, after,...\n",
       "2       [previous, research, has, confirmed, the, inhi...\n",
       "3       [therefore, the, current, study, intends, to, ...\n",
       "4       [the, combined, treatment, significantly, supp...\n",
       "                              ...                        \n",
       "4598    [by, november, december, ba, had, replaced, th...\n",
       "4599    [polymerase, chain, reaction, and, near, full,...\n",
       "4600    [mutations, altering, viral, tropism, replicat...\n",
       "4601    [omicron, ancestors, were, therefore, present,...\n",
       "4602    [these, data, also, indicate, that, travel, ba...\n",
       "Name: inference_sentence, Length: 4603, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retraction_gpt_inference_data[\"inference_sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "retraction_gpt_inference_data.to_parquet(\"data/validation_data/retracted/all_from_29_11_2022.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MLE(f\"distribution/CS.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated, ci=model.inference(\"data/validation_data/retracted/all_from_29_11_2022.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.043"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'OriginalPaperDate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/projects/retractions_nlp/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/dist_gpt_quant_env/lib/python3.8/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/projects/retractions_nlp/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/dist_gpt_quant_env/lib/python3.8/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/projects/retractions_nlp/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/dist_gpt_quant_env/lib/python3.8/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'OriginalPaperDate'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reference_gpr_inferenece_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_gpt_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreference_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m reference_gpt_inference_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(pd\u001b[38;5;241m.\u001b[39mSeries(reference_gpr_inferenece_dataset), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference_sentence\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m reference_gpt_inference_data\u001b[38;5;241m.\u001b[39mto_parquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/validation_data/retracted/reference_from_29_11_2022.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/projects/retractions_nlp/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/data_loader.py:53\u001b[0m, in \u001b[0;36mdataset_gpt_inference\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdataset_gpt_inference\u001b[39m(df):\n\u001b[0;32m---> 53\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginalPaperDate\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOriginalPaperDate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     54\u001b[0m     target_date \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m29.11.2022\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm.\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     55\u001b[0m     df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginalPaperDate\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m target_date]\n",
      "File \u001b[0;32m~/projects/retractions_nlp/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/dist_gpt_quant_env/lib/python3.8/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/projects/retractions_nlp/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/dist_gpt_quant_env/lib/python3.8/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'OriginalPaperDate'"
     ]
    }
   ],
   "source": [
    "reference_gpr_inferenece_dataset = dataset_gpt_inference(reference_df)\n",
    "reference_gpt_inference_data = pd.DataFrame(pd.Series(reference_gpr_inferenece_dataset), columns=[\"inference_sentence\"])\n",
    "reference_gpt_inference_data.to_parquet(\"data/validation_data/retracted/reference_from_29_11_2022.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.042\n"
     ]
    }
   ],
   "source": [
    "estimated, ci=model.inference(\"data/validation_data/retracted/reference_from_29_11_2022.parquet\")\n",
    "print(estimated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dist_gpt_quant_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
